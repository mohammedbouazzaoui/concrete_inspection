{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspection of concrete structures\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "def get_preprocessed_images(images_directory: str, image_size: tuple) -> list:\n",
    "    stop=1000\n",
    "    images = []\n",
    "    for img in os.listdir(images_directory):\n",
    "        img = image.load_img(images_directory+img, target_size=image_size)\n",
    "        img = image.img_to_array(img)\n",
    "        img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))\n",
    "        img = preprocess_input(img)\n",
    "        images.append(img)\n",
    "\n",
    "        if stop == 0:\n",
    "            break\n",
    "        stop-=1\n",
    "        print(stop)\n",
    "    return np.vstack(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the keras preprocessing method.\n",
    "from tensorflow.keras.preprocessing import image\n",
    "SIZE=256\n",
    "image_size = (SIZE, SIZE)\n",
    "# Load your images and preprocess them.\n",
    "cracks_images = get_preprocessed_images(\"C:/Users/bmadmin/Desktop/Octocat/mohammedbouazzaoui/concrete_inspection_dataset/SDNET2018/X/CX_DEMO/\", image_size)\n",
    "non_cracks_images = get_preprocessed_images(\"C:/Users/bmadmin/Desktop/Octocat/mohammedbouazzaoui/concrete_inspection_dataset/SDNET2018/X/UX_DEMO/\", image_size)\n",
    "\n",
    "# Make a numpy array for each of the class labels (one hot encoded).\n",
    "cracks_labels = np.tile([1, 0], (cracks_images.shape[0], 1))\n",
    "non_cracks_labels = np.tile([0, 1], ( non_cracks_images.shape[0], 1))\n",
    "\n",
    "# Concatenate your images and your labels into X and y.\n",
    "X = np.concatenate([cracks_images,  non_cracks_images])\n",
    "y = np.concatenate([cracks_labels,  non_cracks_labels])\n",
    "\n",
    "\n",
    "# TESTING\n",
    "#X = np.concatenate([non_cracks_images])\n",
    "#y = np.concatenate([non_cracks_labels])\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best result uptonow Function\n",
    "def cleanup_img(image_array):\n",
    "    #original_image_array = X[IMAGEPOINT] ### FILL IN ###\n",
    "    original_image_array = image_array\n",
    "\n",
    "    img =  cv2.cvtColor(original_image_array, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    ret,imgshow1 = cv2.threshold(img,0,255,cv2.THRESH_BINARY)\n",
    "\n",
    "    #removing small components\n",
    "\n",
    "\n",
    "    ret, blackAndWhite = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    img2 = blackAndWhite.astype(\"uint8\")\n",
    "    nlabels, labels, stats, centroids = cv2.connectedComponentsWithStats(img2, None, None, None, 8, cv2.CV_32S)\n",
    "    sizes = stats[1:, -1] #get CC_STAT_AREA component\n",
    "    img2 = np.zeros((labels.shape), np.uint8)\n",
    "\n",
    "    a=list(sizes)\n",
    "    a.sort(reverse=True)\n",
    "    #print(\"START A:\",a)\n",
    "    for i in range(len(a)-1):\n",
    "        if a[i] > (20 * a[i+1]):\n",
    "            a[i]=0\n",
    "        else:\n",
    "            break\n",
    "    a.sort(reverse=True)\n",
    "\n",
    "    q=sum(a) / len(a)\n",
    "    if len(a) > 40:\n",
    "        FILTERDOTLOWER = a[int(len(a)/2)]\n",
    "    else:\n",
    "        FILTERDOTLOWER=0\n",
    "\n",
    "    FILTERDOTUPPER = a[0]\n",
    "\n",
    "    FILTERDOTLOWER=FILTERDOTUPPER/10\n",
    "\n",
    "    for i in range(0, nlabels - 1):\n",
    "        if sizes[i] >= FILTERDOTLOWER and sizes[i] <= FILTERDOTUPPER:   #filter small dotted regions\n",
    "            img2[labels == i + 1] = 255\n",
    "            \n",
    "    resimage = cv2.bitwise_not(img2)\n",
    "    ret, resimage = cv2.threshold(resimage, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    return (resimage)\n",
    "\n",
    "###\n",
    "###   GOTO START MODELLING\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XNEW=[]\n",
    "for image_array in X:\n",
    "    #print(image_array)\n",
    "    \n",
    "    clean_image=cleanup_img(image_array)\n",
    "    XNEW.append(clean_image)\n",
    "    #print(type(clean_image),clean_image.shape,clean_image)\n",
    "    \n",
    "X[0]\n",
    "type(XNEW[0])\n",
    "a=XNEW[0]\n",
    "#a.reshape(256,256,3)\n",
    "\n",
    "\n",
    "tX=XNEW[0]\n",
    "#tX = np.random.rand(100, 256, 256)\n",
    "tX.shape\n",
    "\n",
    "tX = np.expand_dims(tX, axis=2)\n",
    "tX = np.expand_dims(tX, axis=2)\n",
    "\n",
    "tX.shape\n",
    "#tX\n",
    "###\n",
    "#X = XNEW  \n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(XNEW[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z=1\n",
    "#res=XNEW[0]\n",
    "#res=X[0]\n",
    "plt.figure(figsize=(10,10))\n",
    "Z+=1\n",
    "plt.subplot(1,2,1),plt.imshow(XNEW[Z],cmap='gray')\n",
    "plt.subplot(1,2,2),plt.imshow(X[Z],cmap='gray')\n",
    "print(y[Z])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING\n",
    "# Display one image\n",
    "from matplotlib import pyplot as plt\n",
    "def display_one(original_img, title1 = \"Original\", isGray=False):\n",
    "    if isGray:\n",
    "        img=image.array_to_img(original_img)\n",
    "        #img = cv2.cvtColor(edited_image, cv2.COLOR_BGR2GRAY)\n",
    "        plt.imshow(img, cmap=\"gray\")\n",
    "    else:\n",
    "        img=image.array_to_img(original_img)\n",
    "        plt.imshow(img)\n",
    "\n",
    "    plt.title(title1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display two images\n",
    "def display_two(original_img, edited_img, title1 = \"Original\", title2 = \"Edited\", isGray=False):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    if isGray:\n",
    "        img=image.array_to_img(original_img)\n",
    "        #img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        plt.imshow(img, cmap=\"gray\") \n",
    "    else:\n",
    "        img=image.array_to_img(original_img)\n",
    "        plt.imshow(img)\n",
    "    plt.title(title1), plt.xticks([]), plt.yticks([])\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    \n",
    "    if isGray:\n",
    "        \n",
    "        #edited_img = cv2.cvtColor(edited_img, cv2.COLOR_BGR2GRAY)\n",
    "        plt.imshow(edited_img, cmap=\"gray\")\n",
    "    else:\n",
    "        print(\"no grey\")\n",
    "        plt.imshow(edited_img)\n",
    "    plt.title(title2), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGEPOINT=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results not good for this !\n",
    "\n",
    "IMAGEPOINT+=1\n",
    "# Resize the image using the cv2.resize() method and linear interpolation\n",
    "import cv2\n",
    "import numpy as np\n",
    "height = SIZE\n",
    "width = SIZE\n",
    "#IMAGEPOINT=99\n",
    "\n",
    "#for i in range(200):\n",
    "print(IMAGEPOINT)\n",
    "original_image_array1 = X[IMAGEPOINT] ### FILL IN ###\n",
    "original_image_array = cv2.cvtColor(original_image_array1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "edited_image = cv2.resize(original_image_array, (width,height),interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "t_lower = 127 # Lower Threshold\n",
    "t_upper = 250 # Upper threshold\n",
    "edited_image=(edited_image*255).astype(np.uint8) \n",
    "edited_image = cv2.Canny(edited_image, t_lower, t_upper)\n",
    "#plt.figure(figsize=(10,20))\n",
    "\n",
    "\n",
    "\n",
    "#plt.imshow(dd, cmap=\"gray\") \n",
    "display_two(original_image_array1, edited_image, title1 = \"Original\", title2 = \"Edited\", isGray=True)\n",
    "\n",
    "#cv2.imwrite('res.png', res)\n",
    "#plt.figure(figsize=(10,10))\n",
    "#plt.imshow(original_image_array1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGEPOINT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best result uptonow\n",
    "\n",
    "IMAGEPOINT+=1\n",
    "\n",
    "print(IMAGEPOINT)\n",
    "\n",
    "original_image_array1 = X[IMAGEPOINT] ### FILL IN ###\n",
    "\n",
    "img =  cv2.cvtColor(original_image_array1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "ret,imgshow1 = cv2.threshold(img,0,255,cv2.THRESH_BINARY)\n",
    "\n",
    "plt.figure(figsize=(25,25))\n",
    "\n",
    "plt.subplot(1,4,1),plt.imshow(original_image_array1,'gray',vmin=0,vmax=255)\n",
    "#plt.subplot(1,4,2),plt.imshow(img,'gray',vmin=0,vmax=255)\n",
    "\n",
    "\n",
    "\n",
    "##############################\n",
    "#We can additionally convert it to grayscale\n",
    "#img_GRAYSCALE =  cv2.cvtColor(original_image_array1, cv2.COLOR_BGR2GRAY)\n",
    "#img=img_GRAYSCALE\n",
    "\n",
    "#img=original_image\n",
    "#ret,imgshow1 = cv.threshold(img,0,255,cv2.THRESH_BINARY_INV)\n",
    "\n",
    "########################################################\n",
    "#removing small components\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "plt.subplot(1,4,2),plt.imshow(img,'gray')\n",
    "#img = img.astype(\"uint8\")\n",
    "\n",
    "\n",
    "ret, blackAndWhite = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV)\n",
    "plt.subplot(1,4,3),plt.imshow(blackAndWhite,cmap='gray')\n",
    "\n",
    "\n",
    "img2 = blackAndWhite.astype(\"uint8\")\n",
    "nlabels, labels, stats, centroids = cv2.connectedComponentsWithStats(img2, None, None, None, 8, cv2.CV_32S)\n",
    "sizes = stats[1:, -1] #get CC_STAT_AREA component\n",
    "img2 = np.zeros((labels.shape), np.uint8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "a=list(sizes)\n",
    "a.sort(reverse=True)\n",
    "#print(\"START A:\",a)\n",
    "for i in range(len(a)-1):\n",
    "    if a[i] > (20 * a[i+1]):\n",
    "        a[i]=0\n",
    "    else:\n",
    "        break\n",
    "a.sort(reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "q=sum(a) / len(a)\n",
    "if len(a) > 40:\n",
    "    FILTERDOTLOWER = a[int(len(a)/2)]\n",
    "else:\n",
    "    FILTERDOTLOWER=0\n",
    "\n",
    "#print(\"START A:\",a)\n",
    "FILTERDOTUPPER = a[0]\n",
    "\n",
    "FILTERDOTLOWER=FILTERDOTUPPER/10\n",
    "print(\"@@###\",FILTERDOTLOWER,FILTERDOTUPPER,a)\n",
    "for i in range(0, nlabels - 1):\n",
    "    if sizes[i] >= FILTERDOTLOWER and sizes[i] <= FILTERDOTUPPER:   #filter small dotted regions\n",
    "        #print(sizes[i])\n",
    "        img2[labels == i + 1] = 255\n",
    "        \n",
    "#print(\"@@@\",FILTERDOT,anz_comp,sizes,img2[1])\n",
    "resimage = cv2.bitwise_not(img2)\n",
    "ret, resimage = cv2.threshold(resimage, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "plt.subplot(1,4,4),plt.imshow(resimage,'gray',vmin=0,vmax=55)\n",
    " \n",
    " \n",
    " \n",
    " \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=list(sizes) \n",
    "z.sort(reverse=True)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blackAndWhite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGEPOINT-=1\n",
    "print(IMAGEPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(a)\n",
    "plt.show()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "#We can additionally convert it to grayscale\n",
    "img_GRAYSCALE =  cv2.cvtColor(original_image_array1, cv2.COLOR_BGR2GRAY)\n",
    "img=img_GRAYSCALE\n",
    "\n",
    "#img=original_image\n",
    "ret,imgshow1 = cv2.threshold(img,0,255,cv2.THRESH_BINARY_INV)\n",
    "#ret,imgshow2 = cv.threshold(img,127,255,cv.THRESH_BINARY_INV)\n",
    "#ret,thresh3 = cv.threshold(img,127,255,cv.THRESH_TRUNC)\n",
    "#ret,thresh4 = cv.threshold(img,127,255,cv.THRESH_TOZERO)\n",
    "#ret,thresh5 = cv.threshold(img,127,255,cv.THRESH_TOZERO_INV)\n",
    "#titles = ['Original Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']\n",
    "#images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]\n",
    "plt.figure(figsize=(30,30))\n",
    "\n",
    "\n",
    "kernel = np.ones((3,3), np.uint8)\n",
    "#imgshow1 = cv2.erode(imgshow1, kernel, iterations=2)\n",
    "#img = cv2.dilate(img, kernel, iterations=1)\n",
    "#ret,imgshow1 = cv.threshold(imgshow1,0,128,cv.THRESH_BINARY_INV) \n",
    "\n",
    "plt.subplot(1,3,1),plt.imshow(original_image_array1)\n",
    "plt.subplot(1,3,2),plt.imshow(imgshow1,'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image_array1.shape\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing small components\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "src = imgshow1\n",
    "img = src.astype(\"uint8\")\n",
    "_, blackAndWhite = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "\n",
    "nlabels, labels, stats, centroids = cv2.connectedComponentsWithStats(blackAndWhite, None, None, None, 8, cv2.CV_32S)\n",
    "sizes = stats[1:, -1] #get CC_STAT_AREA component\n",
    "img2 = np.zeros((labels.shape), np.uint8)\n",
    "\n",
    "for i in range(0, nlabels - 1):\n",
    "    if sizes[i] >= 800:   #filter small dotted regions\n",
    "        img2[labels == i + 1] = 255\n",
    "\n",
    "res = cv2.bitwise_not(img2)\n",
    "\n",
    "#cv2.imwrite('res.png', res)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(res,'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " imgshow1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the cv2 library\n",
    "import cv2\n",
    "# Read the image you want connected components of\n",
    "#img=original_image_array1\n",
    "#a, b, c = cv2.split(img)\n",
    "src = imgshow1\n",
    "src = src.astype(\"uint8\")\n",
    "# Threshold it so it becomes binary\n",
    " \n",
    "ret, thresh = cv2.threshold(src,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "# You need to choose 4 or 8 for connectivity type\n",
    "connectivity = 4  \n",
    "# Perform the operation\n",
    "output = cv2.connectedComponentsWithStats(thresh, connectivity, cv2.CV_32S)\n",
    "# Get the results\n",
    "# The first cell is the number of labels\n",
    "num_labels = output[0]\n",
    "# The second cell is the label matrix\n",
    "labels = output[1]\n",
    "# The third cell is the stat matrix\n",
    "stats = output[2]\n",
    "# The fourth cell is the centroid matrix\n",
    "centroids = output[3]\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input()\n",
    "#break\n",
    "\n",
    "#Test erosion/dilation\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Reading the input image\n",
    "img = edited_image\n",
    " \n",
    "kernel = np.ones((2,2), np.uint8)\n",
    "#img = cv2.erode(img, kernel, iterations=1)\n",
    "#img = cv2.dilate(img, kernel, iterations=1)\n",
    "imgshow=img\n",
    " \n",
    "\n",
    "\n",
    "#cv2.imshow('origin', img)\n",
    "#cv2.imshow('img_erode', img_erode)\n",
    "#cv2.imshow('img_dilate', img_dilate)\n",
    "#cv2.imshow('img_erode2', img_erode2)\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(imgshow,'gray')\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test own filter\n",
    " \n",
    "original_image_array1 = X[IMAGEPOINT] ### FILL IN ###\n",
    "#original_image_array = cv2.cvtColor(original_image_array1, cv2.COLOR_BGR2GRAY)\n",
    "#edited_image = cv2.resize(original_image_array, (width,height),interpolation=cv2.INTER_LINEAR)\n",
    "imgo=original_image_array1.copy()\n",
    "imgo=(imgo*255).astype(np.uint8)\n",
    "\n",
    "def lijnx(y,img_array):\n",
    "    shp=img_array.shape\n",
    "    z=shp[2]\n",
    "    #y=shp[1]\n",
    "    x=shp[0]\n",
    "    for px in range(0,x):\n",
    "        img_array[y][px][0]=np.uint8(255) \n",
    "        img_array[y][px][1]=np.uint8(255) \n",
    "        img_array[y][px][2]=np.uint8(255) \n",
    "    return img_array\n",
    "\n",
    "def filter1(img_array,depth):\n",
    "    shp=img_array.shape\n",
    "    z=shp[2]\n",
    "    y=shp[1]\n",
    "    x=shp[0]\n",
    "    #print(x,y,z)\n",
    "    for py in range(y):\n",
    "        for px in range(x):\n",
    "            if (img_array[py][px][0] > depth) or (img_array[py][px][1] > depth) or (img_array[py][px][2] > depth) :\n",
    "                img_array[py][px][0]=255\n",
    "                img_array[py][px][1]=255\n",
    "                img_array[py][px][2]=255\n",
    "    return img_array\n",
    "\n",
    "'''\n",
    "#img = lijnx(2,imgo)\n",
    "for depth in range(0,200,10):\n",
    "    imgo=original_image_array1.copy()\n",
    "    imgo=(imgo*255).astype(np.uint8) \n",
    "\n",
    "    img = filter1(imgo,depth)\n",
    "\n",
    " \n",
    "\n",
    "    img=image.array_to_img(imgo)\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "\n",
    "'''   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imgo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unsharp_mask(image, kernel_size=(1, 1), sigma=1.0, amount=1.0, threshold=0):\n",
    "    \"\"\"Return a sharpened version of the image, using an unsharp mask.\n",
    "    \n",
    "    Taken from https://stackoverflow.com/a/55590133\n",
    "    \"\"\"\n",
    "    blurred = cv2.GaussianBlur(image, kernel_size, sigma)\n",
    "    sharpened = float(amount + 1) * image - float(amount) * blurred\n",
    "    sharpened = np.maximum(sharpened, np.zeros(sharpened.shape))\n",
    "    sharpened = np.minimum(sharpened, 255 * np.ones(sharpened.shape))\n",
    "    sharpened = sharpened.round().astype(np.uint8)\n",
    "    if threshold > 0:\n",
    "        low_contrast_mask = np.absolute(image - blurred) < threshold\n",
    "        np.copyto(sharpened, image, where=low_contrast_mask)\n",
    "    return sharpened\n",
    "\n",
    "edited_image = unsharp_mask(img, amount=5.0)\n",
    "#display_two(original_image, edited_image)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(edited_image)\n",
    "print(edited_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#IMAGEPOINT=7\n",
    "original_image = X[IMAGEPOINT-1] ### FILL IN ###\n",
    "# WARNING: openCV works with the BGR(blue, green, red) color map, whereas pyplot works with RGB\n",
    "\n",
    "#img=original_image\n",
    "#We can additionally convert it to grayscale\n",
    "img_GRAYSCALE =  cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n",
    "img=img_GRAYSCALE\n",
    "\n",
    "#img=original_image\n",
    "ret,thresh1 = cv.threshold(img,0,255,cv.THRESH_BINARY)\n",
    "ret,thresh2 = cv.threshold(img,0,255,cv.THRESH_BINARY_INV)\n",
    "\n",
    "img = thresh2\n",
    "kernel = np.ones((2,2), np.uint8)\n",
    "thresh2 = cv2.dilate(img, kernel, iterations=5)\n",
    "\n",
    "\n",
    "#ret,thresh3 = cv.threshold(img,127,255,cv.THRESH_TRUNC)\n",
    "#ret,thresh4 = cv.threshold(img,127,255,cv.THRESH_TOZERO)\n",
    "#ret,thresh5 = cv.threshold(img,127,255,cv.THRESH_TOZERO_INV)\n",
    "titles = ['Original Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']\n",
    "#images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]\n",
    "plt.figure(figsize=(15,15))\n",
    "'''\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1),plt.imshow(images[i],'gray',vmin=0,vmax=255)\n",
    "    plt.title(titles[i])\n",
    "    plt.xticks([]),plt.yticks([])\n",
    "'''\n",
    "plt.subplot(2,3,1),plt.imshow(original_image,'gray',vmin=0,vmax=255)\n",
    "plt.title(titles[0])\n",
    "#plt.subplot(2,3,2),plt.imshow(thresh1,'gray',vmin=0,vmax=255)\n",
    "#plt.title(titles[1])\n",
    "plt.subplot(2,3,3),plt.imshow(thresh2,'gray',vmin=0,vmax=55)\n",
    "plt.title(titles[2])\n",
    "plt.xticks([]),plt.yticks([])\n",
    "plt.show()\n",
    "print(thresh2)\n",
    "original_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best result uptonow Function\n",
    "def cleanup_img2(image_array):\n",
    "    #original_image_array = X[IMAGEPOINT] ### FILL IN ###\n",
    "    original_image_array = image_array\n",
    "\n",
    "    img =  cv2.cvtColor(original_image_array, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    ret,imgshow1 = cv2.threshold(img,0,255,cv2.THRESH_BINARY)\n",
    "\n",
    "    #removing small components\n",
    "\n",
    "\n",
    "    ret, blackAndWhite = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    img2 = blackAndWhite.astype(\"uint8\")\n",
    "    nlabels, labels, stats, centroids = cv2.connectedComponentsWithStats(img2, None, None, None, 8, cv2.CV_32S)\n",
    "    sizes = stats[1:, -1] #get CC_STAT_AREA component\n",
    "    img2 = np.zeros((labels.shape), np.uint8)\n",
    "\n",
    "    a=list(sizes)\n",
    "    if a == []:\n",
    "        return img\n",
    "    a.sort(reverse=True)\n",
    "    #print(\"START A:\",a,len(image_array))\n",
    "    for i in range(len(a)-1):\n",
    "        if a[i] > (20 * a[i+1]):\n",
    "            a[i]=0\n",
    "        else:\n",
    "            break\n",
    "    a.sort(reverse=True)\n",
    "\n",
    "    #q=sum(a) / len(a)\n",
    "    if len(a) > 40:\n",
    "        FILTERDOTLOWER = a[int(len(a)/2)]\n",
    "    else:\n",
    "        FILTERDOTLOWER=0\n",
    "\n",
    "    FILTERDOTUPPER = a[0]\n",
    "\n",
    "    FILTERDOTLOWER=FILTERDOTUPPER/10\n",
    "\n",
    "    for i in range(0, nlabels - 1):\n",
    "        if sizes[i] >= FILTERDOTLOWER and sizes[i] <= FILTERDOTUPPER:   #filter small dotted regions\n",
    "            img2[labels == i + 1] = 255\n",
    "            \n",
    "    resimage = cv2.bitwise_not(img2)\n",
    "    ret, resimage = cv2.threshold(resimage, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    return (resimage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best result uptonow Function\n",
    "def cleanup_img3(image_array):\n",
    "    #original_image_array = X[IMAGEPOINT] ### FILL IN ###\n",
    "    original_image_array = image_array\n",
    "\n",
    "    img =  cv2.cvtColor(original_image_array, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #ret,imgshow1 = cv2.threshold(img,0,255,cv2.THRESH_BINARY)\n",
    "\n",
    "    #removing small components\n",
    "\n",
    "\n",
    "    ret, blackAndWhite = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    #dilate\n",
    "    kernel = np.ones((2,2), np.uint8)\n",
    "    blackAndWhite = cv2.dilate(blackAndWhite, kernel, iterations=3)\n",
    "\n",
    "\n",
    "    #decompose\n",
    "    img2 = blackAndWhite.astype(\"uint8\")\n",
    "    nlabels, labels, stats, centroids = cv2.connectedComponentsWithStats(img2, None, None, None, 8, cv2.CV_32S)\n",
    "    sizes = stats[1:, -1] #get CC_STAT_AREA component\n",
    "    img2 = np.zeros((labels.shape), np.uint8)\n",
    "\n",
    "\n",
    "    a=list(sizes)\n",
    "\n",
    "    if a == []:\n",
    "        return img\n",
    "        \n",
    "    #print(a)\n",
    "    a.sort(reverse=True)\n",
    "    #print(\"START A:\",a)\n",
    "    \n",
    "    for i in range(len(a)-1):\n",
    "        if a[i] > (20 * a[i+1]):\n",
    "            a[i]=0\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    a.sort(reverse=True)\n",
    "\n",
    "    #q=sum(a) / len(a)\n",
    "    #if len(a) > 40:\n",
    "    #    FILTERDOTLOWER = a[int(len(a)/2)]\n",
    "    #else:\n",
    "    #    FILTERDOTLOWER=0\n",
    "\n",
    "\n",
    "    FILTERDOTUPPER = a[0]\n",
    "\n",
    "    FILTERDOTLOWER=FILTERDOTUPPER/5\n",
    "\n",
    "    for i in range(0, nlabels - 1):\n",
    "        if sizes[i] >= FILTERDOTLOWER and sizes[i] <= FILTERDOTUPPER:   #filter small dotted regions\n",
    "            img2[labels == i + 1] = 255\n",
    "            \n",
    "    resimage = cv2.bitwise_not(img2)\n",
    "    ret, resimage = cv2.threshold(resimage, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    kernel = np.ones((2,2), np.uint8)\n",
    "    resimage = cv2.dilate(resimage, kernel, iterations=3)\n",
    "    resimage = cv2.erode(resimage, kernel, iterations=3)\n",
    "\n",
    "\n",
    "\n",
    "    return (resimage)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### DO CLEANING\n",
    "XNEW=[]\n",
    "for image_array in X:\n",
    "    #print(image_array)\n",
    "    \n",
    "    clean_image=cleanup_img3(image_array)\n",
    "    XNEW.append(clean_image/255)\n",
    "XNEW = np.array(XNEW)\n",
    "imgcnt=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR DEMO PRESENTATION\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "#X.shape,XNEW.shape, X[10].shape , XNEW[10].shape\n",
    "#XNEW[10][0:2].shape, X[10][0:2].shape\n",
    "imgcnt+=1\n",
    "#row=0\n",
    "\n",
    "ONE=np.ones((256,256))\n",
    "ONE3=np.ones((256,256,3))\n",
    "Z = XNEW[imgcnt,:,:]\n",
    "REVERSE=ONE - Z\n",
    "#REVERSE=Z\n",
    "ONE3[:,:,0]=REVERSE\n",
    "ONE3[:,:,1]=REVERSE\n",
    "ONE3[:,:,2]=REVERSE\n",
    "XIM=ONE3 * X[imgcnt]\n",
    "\n",
    "#depth=0\n",
    "#print(ONE.shape,X[img,:,:,depth].shape , XNEW[img,:,:].shape)\n",
    "\n",
    "#XNEW[img,row,:] , X[img,row,:,depth]\n",
    "\n",
    "ONE3[0], REVERSE[0]\n",
    "XIM[0]\n",
    "print(imgcnt)\n",
    "plt.figure(figsize=(25,25))\n",
    "plt.subplot(1,3,1),plt.imshow(X[imgcnt])\n",
    "plt.subplot(1,3,2),plt.imshow(ONE3)\n",
    "plt.subplot(1,3,3),plt.imshow(XIM)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "####\n",
    "#### START MODELLING\n",
    "####\n",
    "#########################\n",
    "'''\n",
    "### DO CLEANING\n",
    "XNEW=[]\n",
    "for image_array in X:\n",
    "    #print(image_array)\n",
    "\n",
    "    clean_image=cleanup_img2(image_array)\n",
    "    XNEW.append(clean_image/255)\n",
    "XNEW = np.array(XNEW)\n",
    "'''\n",
    "######################\n",
    "XRES=[]\n",
    "for image_array in X:\n",
    "    clean_image=cleanup_img2(image_array)\n",
    "    XNEW = clean_image/255\n",
    "\n",
    "    ONE=np.ones((256,256))\n",
    "    ONE3=np.ones((256,256,3))\n",
    "    Z = XNEW\n",
    "    REVERSE=ONE - Z\n",
    "    ONE3[:,:,0]=REVERSE\n",
    "    ONE3[:,:,1]=REVERSE\n",
    "    ONE3[:,:,2]=REVERSE\n",
    "    XIM=ONE3 * X[imgcnt]\n",
    "    XIM=cv2.resize(XIM, (244,244),interpolation=cv2.INTER_LINEAR)\n",
    "    XRES.append(XIM)\n",
    "XRES = np.array(XRES)\n",
    "\n",
    "\n",
    "\n",
    "##########################\n",
    "\n",
    "\n",
    "### TRAIN MODEL\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    XRES, \n",
    "    y,\n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, \n",
    "    y_train_val,\n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = np.expand_dims(X, axis=3)\n",
    "\n",
    "XRES.shape,X_train.shape,X_val.shape,y_train.shape,y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Determine the number of generated samples you want per original sample.\n",
    "datagen_batch_size = 16\n",
    "\n",
    "# Make a datagenerator object using ImageDataGenerator.\n",
    "train_datagen = ImageDataGenerator(rotation_range=60,\n",
    "                                    horizontal_flip=True)\n",
    "\n",
    "# Feed the generator your train data.\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=datagen_batch_size)\n",
    "\n",
    "# Make a datagenerator object using ImageDataGenerator.\n",
    "validation_datagen = ImageDataGenerator(rotation_range=60,\n",
    "                                        horizontal_flip=True)\n",
    "\n",
    "# Feed the generator your validation data.\n",
    "validation_generator = validation_datagen.flow(X_val, y_val, batch_size=datagen_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your chosen model!\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "\n",
    "# Make a model object. \n",
    "# Make sure you exclude the top part. set the input shape of the model to 224x224 pixels, with 3 color channels.\n",
    "#model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "\n",
    "# Freeze the imported layers so they cannot be retrained.\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "\n",
    "new_model = Sequential()\n",
    "new_model.add(model)\n",
    "new_model.add(Flatten())\n",
    "new_model.add(Dense(64, activation='relu'))\n",
    "new_model.add(Dropout(0.5))\n",
    "new_model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "# Summarize.\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit the model. Use the Adam optimizer and crossentropical loss. \n",
    "# Use the validation data argument during fitting to include your validation data.\n",
    "#new_model.compile(optimizer='adam',\n",
    "#                  loss='binary_crossentropy',\n",
    "#                  metrics=['accuracy'])\n",
    "import tensorflow as tf\n",
    "\n",
    "new_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(),\n",
    "                       tf.keras.metrics.FalseNegatives()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = new_model.fit(train_generator,\n",
    "                        epochs=10,   #10\n",
    "                        batch_size=8,  #8\n",
    "                        validation_data=validation_generator\n",
    "                       )\n",
    "new_model.save('../pybin/models/activemodel9999')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import tensorflow\n",
    "\n",
    "def plot_history(history : tensorflow.keras.callbacks.History):\n",
    "    \"\"\" This helper function takes the tensorflow.python.keras.callbacks.History\n",
    "    that is output from your `fit` method to plot the loss and accuracy of\n",
    "    the training and validation set.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1,3, figsize=(22,6))\n",
    "    axs[0].plot(history.history['binary_accuracy'], label='training')\n",
    "    axs[0].plot(history.history['val_binary_accuracy'], label='validation')\n",
    "    axs[0].set(xlabel = 'Epoch', ylabel='Binary accuracy', ylim=[0, 1])\n",
    "    axs[0].legend(loc='lower right')\n",
    "\n",
    "    axs[1].plot(history.history['loss'], label='training')\n",
    "    axs[1].plot(history.history['val_loss'], label = 'validation')\n",
    "    axs[1].set(xlabel = 'Epoch', ylabel='Loss', ylim=[0, 1])\n",
    "    axs[1].legend(loc='lower right')\n",
    "\n",
    "    axs[2].plot(history.history['false_negatives_2'], label = 'training')\n",
    "    axs[2].plot(history.history['val_false_negatives_2'], label = 'validation')\n",
    "    axs[2].set(xlabel = 'Epoch', ylabel='false_negatives', ylim=[0, 500])\n",
    "    axs[2].legend(loc='lower right')\n",
    "\n",
    "\n",
    "    \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z=98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Z-=1\n",
    "if Z == 1:\n",
    "    Z=98\n",
    "\n",
    "# Load your image. Make sure it is loaded in with the right dimensions for your model!\n",
    "#image_size = (224, 224)\n",
    "#original_image = image.load_img(\"C:/Users/bmadmin/Desktop/Octocat/mohammedbouazzaoui/concrete_inspection/project/data/AAAA.jpg\", target_size=image_size)\n",
    "\n",
    "with open('C:/Users/bmadmin/Desktop/Octocat/mohammedbouazzaoui/concrete_inspection_dataset/BACKUP_DATA/Chundata/filelist.txt', 'rt') as f:\n",
    "    for i in range(Z):\n",
    "        file=f.readline()\n",
    "        #print(file)\n",
    "file=file.strip('\\n')\n",
    "\n",
    "original_image = image.load_img(file, target_size=image_size)\n",
    "\n",
    "# Convert your image pixels to a numpy array of values .\n",
    "image_array = image.img_to_array(original_image)\n",
    "\n",
    "# Reshape your image dimensions so that the colour channels correspond to what your model expects.\n",
    "image_array = image_array.reshape((1, image_array.shape[0], image_array.shape[1], image_array.shape[2]))\n",
    "\n",
    "# Preprocess your image with preprocess_input.\n",
    "prepared_image = preprocess_input(image_array)\n",
    "\n",
    "# Predict the class of your picture.\n",
    "prediction = new_model.predict(prepared_image)\n",
    "\n",
    "# Print out your result.\n",
    "print(prediction)\n",
    "if prediction[0][0] > prediction[0][1]:\n",
    "    print(\"#### >>>> CRACK\")\n",
    "else:\n",
    "    print(\"#### >>>> NOCRACK\")\n",
    "# Show handsome rick.\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "plt.imshow(original_image)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "###\n",
    "###  END\n",
    "###\n",
    "#######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "histo=np.load('C:/Users/bmadmin/Desktop/Octocat/mohammedbouazzaoui/concrete_inspection/project/static/history.npy',allow_pickle=True)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [7.50, 3.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "x = np.array([5, 4, 1, 4, 5])\n",
    "y = np.sort(x)\n",
    "\n",
    "plt.title(\"Line graph\")\n",
    "plt.plot(x, y, color=\"red\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "43555e2ca980a29bec6bc428ca6c1f56020b40b98b62f58def2ce1400820b105"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
